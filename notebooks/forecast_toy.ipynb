{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewalt/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil\n",
    "import xarray as xr\n",
    "import torch\n",
    "import dask\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from aurora import Aurora, AuroraSmall, rollout\n",
    "\n",
    "from aurora_benchmark.utils import verbose_print, xr_to_netcdf\n",
    "\n",
    "from aurora_benchmark.data import (\n",
    "    XRAuroraDataset, \n",
    "    XRAuroraBatchedDataset,\n",
    "    aurora_batch_collate_fn, \n",
    "    aurora_batch_to_xr, \n",
    "    unpack_aurora_batch\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Suppress logs from Google libraries\n",
    "logging.getLogger('google').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.auth').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.cloud').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenMappingWarningOnValuesAccess({'time': 365, 'longitude': 360, 'latitude': 180}),\n",
       " FrozenMappingWarningOnValuesAccess({'time': 365, 'longitude': 360, 'latitude': 180, 'level': 13}),\n",
       " FrozenMappingWarningOnValuesAccess({'latitude': 180, 'longitude': 360}))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era5_surface_paths = [\n",
    "    \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/t2m-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/u10-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/v10-2021-2022-1d-360x180.nc\",\n",
    "]\n",
    "era5_atmospheric_paths = [ \n",
    "    \"../toy_data/era5-1d-360x180/t-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/q-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/u-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/v-2021-2022-1d-360x180.nc\",\n",
    "    \"../toy_data/era5-1d-360x180/z-2021-2022-1d-360x180.nc\",\n",
    "]\n",
    "era5_static_paths = [\n",
    "    \"/projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2/2021-2022-6h-1440x721/lsm_static-1440x721.nc\",\n",
    "    \"/projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2/2021-2022-6h-1440x721/z_static-1440x721.nc\",\n",
    "    \"/projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2/2021-2022-6h-1440x721/slt_static-1440x721.nc\",\n",
    "]\n",
    "\n",
    "# Load the data into a single dataset with the same coords but multiple variables\n",
    "surface_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar})#.drop_vars(\"time_bnds\")\n",
    "    for path in era5_surface_paths\n",
    "]\n",
    "surface_ds = xr.merge(surface_dss).rename({\"t2m\": \"2t\", \"u10\": \"10u\", \"v10\": \"10v\", \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "atmospheric_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360, \"level\": 1}).rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]})#.drop_vars(\"time_bnds\")\n",
    "    for path in era5_atmospheric_paths\n",
    "]\n",
    "atmospheric_ds = xr.merge(atmospheric_dss).rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "static_dss = [\n",
    "     xr.open_dataset(path, engine=\"netcdf4\").coarsen(longitude=1440//360, latitude=721//180, boundary=\"trim\").mean()\n",
    "    # xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar}).isel(time=0)#.drop_vars(\"time_bnds\")\n",
    "    for path in era5_static_paths\n",
    "]\n",
    "static_ds = xr.merge(static_dss)\n",
    "\n",
    "surface_ds.dims, atmospheric_ds.dims, static_ds.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:22:26,890 - aurora_benchmark.utils - INFO - Using dask scheduler: threads\n",
      "2024-10-16 11:22:26,890 - aurora_benchmark.utils - INFO - Creating XRAuroraBatchedDataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:22:26,899 - aurora_benchmark.utils - INFO - Dataset length: 42\n",
      "2024-10-16 11:22:26,899 - aurora_benchmark.utils - INFO - Dataloader length: 3 (type: <class 'aurora_benchmark.data.XRAuroraBatchedDataset'>, batch_size: 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2_forecasts/2021-2022-1d-1w-10w-360x180_original_variables/\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "eval_start =\"1w\"\n",
    "era5_base_frequency = \"1d\"\n",
    "forecast_horizon = \"10w\"\n",
    "use_dataloader = False\n",
    "eval_aggregation = \"1w\"\n",
    "init_frequency = \"1w\"\n",
    "verbose = True\n",
    "drop_timestamps = False\n",
    "persist = False\n",
    "rechunk = False\n",
    "output_dir = \"/projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2_forecasts/2021-2022-1d-1w-10w-360x180_original_variables/\"\n",
    "\n",
    "surf_vars = [\"2t\", \"msl\", \"10u\", \"10v\"]\n",
    "atmospheric_vars = [\"t\", \"q\", \"z\", \"u\"]\n",
    "static_vars = [\"z\", \"lsm\", \"slt\"]\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "warmup_steps = int(pd.Timedelta(eval_start) / pd.Timedelta(era5_base_frequency)) if eval_start is not None else 0\n",
    "forecast_steps = int(pd.Timedelta(forecast_horizon) / pd.Timedelta(era5_base_frequency))\n",
    "\n",
    "assert (forecast_steps-warmup_steps) * pd.Timedelta(era5_base_frequency) >= pd.Timedelta(eval_aggregation), \"Evaluation steps must be at least as long as eval_aggregation\" \n",
    "\n",
    "if use_dataloader:\n",
    "    dask.config.set(scheduler='synchronous')\n",
    "else:\n",
    "    dask.config.set(scheduler='threads')\n",
    "verbose_print(verbose, f\"Using dask scheduler: {dask.config.get('scheduler')}\")\n",
    "\n",
    "\n",
    "if use_dataloader:\n",
    "    verbose_print(verbose, f\"Creating XRAuroraDataset and DataLoader...\")\n",
    "    dataset = XRAuroraDataset(\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    verbose_print(verbose, f\"Loaded dataset of length {len(dataset)} (drop_timestamps={drop_timestamps}, persist={persist}, rechunk={rechunk})\")\n",
    "    \n",
    "    num_workers = 2 #int(os.getenv('SLURM_CPUS_PER_TASK', 1))+2 if os.getenv('SLURM_CPUS_PER_TASK') is not None else os.cpu_count()+2\n",
    "    verbose_print(verbose, f\"Creating DataLoader with {num_workers} workers ...\")\n",
    "    eval_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=aurora_batch_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    batch_iterator = eval_loader\n",
    "else:\n",
    "    # This is done to avoid the issue with torch DataLoader and dask\n",
    "    # when using netcdf files (i.e. netcdf backend is not thread safe)\n",
    "    verbose_print(verbose, f\"Creating XRAuroraBatchedDataset ...\")\n",
    "    dataset = XRAuroraBatchedDataset(\n",
    "        batch_size=batch_size,\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    batch_iterator = dataset\n",
    "\n",
    "verbose_print(verbose, f\"Dataset length: {dataset.flat_length() if hasattr(dataset, 'flat_length') else len(dataset)}\")\n",
    "verbose_print(verbose, f\"Dataloader length: {len(batch_iterator)} (type: {type(batch_iterator)}, batch_size: {batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:22:42,887 - aurora_benchmark.utils - INFO - Evaluating on cuda\n",
      "2024-10-16 11:22:43,626 - aurora_benchmark.utils - INFO - Rollout prediction on batch 0 ...\n",
      "2024-10-16 11:22:47,246 - aurora_benchmark.utils - INFO -  * Rollout step 1: skipping warmup period\n",
      "2024-10-16 11:22:50,400 - aurora_benchmark.utils - INFO -  * Rollout step 2: skipping warmup period\n",
      "2024-10-16 11:22:53,551 - aurora_benchmark.utils - INFO -  * Rollout step 3: skipping warmup period\n",
      "2024-10-16 11:22:56,701 - aurora_benchmark.utils - INFO -  * Rollout step 4: skipping warmup period\n",
      "2024-10-16 11:22:59,852 - aurora_benchmark.utils - INFO -  * Rollout step 5: skipping warmup period\n",
      "2024-10-16 11:23:03,003 - aurora_benchmark.utils - INFO -  * Rollout step 6: skipping warmup period\n",
      "2024-10-16 11:23:06,155 - aurora_benchmark.utils - INFO -  * Rollout step 7: skipping warmup period\n",
      "2024-10-16 11:23:09,327 - aurora_benchmark.utils - INFO -  * Rollout step 8: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:12,510 - aurora_benchmark.utils - INFO -  * Rollout step 9: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:15,694 - aurora_benchmark.utils - INFO -  * Rollout step 10: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:18,876 - aurora_benchmark.utils - INFO -  * Rollout step 11: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:22,061 - aurora_benchmark.utils - INFO -  * Rollout step 12: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:25,244 - aurora_benchmark.utils - INFO -  * Rollout step 13: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:28,424 - aurora_benchmark.utils - INFO -  * Rollout step 14: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:31,626 - aurora_benchmark.utils - INFO -  * Rollout step 15: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:34,831 - aurora_benchmark.utils - INFO -  * Rollout step 16: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:38,011 - aurora_benchmark.utils - INFO -  * Rollout step 17: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:41,187 - aurora_benchmark.utils - INFO -  * Rollout step 18: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:44,362 - aurora_benchmark.utils - INFO -  * Rollout step 19: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:47,538 - aurora_benchmark.utils - INFO -  * Rollout step 20: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:50,715 - aurora_benchmark.utils - INFO -  * Rollout step 21: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:53,901 - aurora_benchmark.utils - INFO -  * Rollout step 22: unpacked 16 sub-batches\n",
      "2024-10-16 11:23:57,078 - aurora_benchmark.utils - INFO -  * Rollout step 23: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:00,254 - aurora_benchmark.utils - INFO -  * Rollout step 24: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:03,430 - aurora_benchmark.utils - INFO -  * Rollout step 25: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:06,605 - aurora_benchmark.utils - INFO -  * Rollout step 26: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:09,783 - aurora_benchmark.utils - INFO -  * Rollout step 27: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:12,960 - aurora_benchmark.utils - INFO -  * Rollout step 28: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:16,134 - aurora_benchmark.utils - INFO -  * Rollout step 29: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:19,310 - aurora_benchmark.utils - INFO -  * Rollout step 30: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:22,484 - aurora_benchmark.utils - INFO -  * Rollout step 31: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:25,662 - aurora_benchmark.utils - INFO -  * Rollout step 32: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:28,838 - aurora_benchmark.utils - INFO -  * Rollout step 33: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:32,013 - aurora_benchmark.utils - INFO -  * Rollout step 34: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:35,188 - aurora_benchmark.utils - INFO -  * Rollout step 35: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:38,363 - aurora_benchmark.utils - INFO -  * Rollout step 36: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:41,539 - aurora_benchmark.utils - INFO -  * Rollout step 37: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:44,719 - aurora_benchmark.utils - INFO -  * Rollout step 38: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:47,916 - aurora_benchmark.utils - INFO -  * Rollout step 39: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:51,092 - aurora_benchmark.utils - INFO -  * Rollout step 40: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:54,265 - aurora_benchmark.utils - INFO -  * Rollout step 41: unpacked 16 sub-batches\n",
      "2024-10-16 11:24:57,441 - aurora_benchmark.utils - INFO -  * Rollout step 42: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:00,639 - aurora_benchmark.utils - INFO -  * Rollout step 43: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:03,814 - aurora_benchmark.utils - INFO -  * Rollout step 44: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:06,991 - aurora_benchmark.utils - INFO -  * Rollout step 45: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:10,166 - aurora_benchmark.utils - INFO -  * Rollout step 46: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:13,340 - aurora_benchmark.utils - INFO -  * Rollout step 47: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:16,519 - aurora_benchmark.utils - INFO -  * Rollout step 48: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:19,694 - aurora_benchmark.utils - INFO -  * Rollout step 49: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:22,869 - aurora_benchmark.utils - INFO -  * Rollout step 50: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:26,043 - aurora_benchmark.utils - INFO -  * Rollout step 51: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:29,220 - aurora_benchmark.utils - INFO -  * Rollout step 52: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:32,394 - aurora_benchmark.utils - INFO -  * Rollout step 53: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:35,572 - aurora_benchmark.utils - INFO -  * Rollout step 54: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:38,750 - aurora_benchmark.utils - INFO -  * Rollout step 55: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:41,926 - aurora_benchmark.utils - INFO -  * Rollout step 56: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:45,103 - aurora_benchmark.utils - INFO -  * Rollout step 57: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:48,278 - aurora_benchmark.utils - INFO -  * Rollout step 58: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:51,456 - aurora_benchmark.utils - INFO -  * Rollout step 59: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:54,652 - aurora_benchmark.utils - INFO -  * Rollout step 60: unpacked 16 sub-batches\n",
      "2024-10-16 11:25:57,828 - aurora_benchmark.utils - INFO -  * Rollout step 61: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:01,022 - aurora_benchmark.utils - INFO -  * Rollout step 62: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:04,201 - aurora_benchmark.utils - INFO -  * Rollout step 63: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:07,381 - aurora_benchmark.utils - INFO -  * Rollout step 64: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:10,556 - aurora_benchmark.utils - INFO -  * Rollout step 65: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:13,732 - aurora_benchmark.utils - INFO -  * Rollout step 66: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:16,906 - aurora_benchmark.utils - INFO -  * Rollout step 67: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:20,081 - aurora_benchmark.utils - INFO -  * Rollout step 68: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:23,255 - aurora_benchmark.utils - INFO -  * Rollout step 69: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:26,434 - aurora_benchmark.utils - INFO -  * Rollout step 70: unpacked 16 sub-batches\n",
      "2024-10-16 11:26:26,436 - aurora_benchmark.utils - INFO - Processing trajectories ...\n",
      "2024-10-16 11:26:26,436 - aurora_benchmark.utils - INFO -  * init_time=2021-01-02 09:00:00: combining 63 steps\n"
     ]
    }
   ],
   "source": [
    "AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {\n",
    "        \"u10\": \"10u\",\n",
    "        \"v10\": \"10v\",\n",
    "        \"t2m\": \"2t\",\n",
    "    },\n",
    "    \"atmospheric\": {},\n",
    "    \"static\": {},\n",
    "}\n",
    "INVERTED_AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"surface\"].items()},\n",
    "    \"atmospheric\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"atmospheric\"].items()},\n",
    "    \"static\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"static\"].items()},\n",
    "}\n",
    "\n",
    "interest_variables = atmospheric_vars + surf_vars\n",
    "interest_levels = [1000, 700, 250]\n",
    "\n",
    "\n",
    "verbose = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"loading model ...\")\n",
    "aurora_model = \"aurora-0.25-pretrained.ckpt\"\n",
    "model = Aurora(use_lora=False)\n",
    "model.load_checkpoint(\"microsoft/aurora\", aurora_model)\n",
    "model = model.to(device)\n",
    "\n",
    "verbose_print(verbose, f\"Evaluating on {device}\")\n",
    "# evaluation loop\n",
    "with torch.inference_mode() and torch.no_grad():\n",
    "    for i, batch in enumerate(batch_iterator):\n",
    "        verbose_print(verbose,f\"Rollout prediction on batch {i} ...\")\n",
    "        if batch is None: continue\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        # rollout until for forecast_steps\n",
    "        trajectories = [[] for _ in range(batch_size)]\n",
    "        for s, batch_pred in enumerate(rollout(model, batch, steps=forecast_steps)):\n",
    "            if s < warmup_steps:\n",
    "                verbose_print(verbose,f\" * Rollout step {s+1}: skipping warmup period\")\n",
    "                continue            \n",
    "            # separate batched batches\n",
    "            sub_batch_preds = unpack_aurora_batch(batch_pred.to(\"cpu\"))\n",
    "            verbose_print(verbose,f\" * Rollout step {s+1}: unpacked {len(sub_batch_preds)} sub-batches\")\n",
    "            if i != len(batch_iterator) - 1: # the last batch may not be full\n",
    "                assert len(sub_batch_preds) == batch_size\n",
    "            # accumulate\n",
    "            for b, sub_batch_pred in enumerate(sub_batch_preds):\n",
    "                trajectories[b].append(sub_batch_pred)\n",
    "        \n",
    "        # convert to xr and process\n",
    "        verbose_print(verbose,f\"Processing trajectories ...\")\n",
    "        for init_time, trajectory in zip(batch.metadata.time, trajectories):\n",
    "            verbose_print(verbose,f\" * init_time={init_time}: combining {len(trajectory)} steps\")\n",
    "            assert len(trajectory) == forecast_steps-warmup_steps\n",
    "            # collate trajectory batches\n",
    "            trajectory = aurora_batch_collate_fn(trajectory)\n",
    "            # convert to xr.Dataset\n",
    "            trajectory = aurora_batch_to_xr(trajectory, frequency=era5_base_frequency)\n",
    "            \n",
    "            # process individual trajectory elements (i.e. variable types)\n",
    "            for var_type, vars_ds in trajectory.items():\n",
    "                # ensure processing is necessary\n",
    "                if var_type == \"static_ds\":\n",
    "                    verbose_print(verbose,f\" * Skipping static variables\")\n",
    "                    continue # we do not care about static variables for the forecast\n",
    "                if not any([var in vars_ds.data_vars for var in interest_variables]):\n",
    "                    verbose_print(verbose,f\" * Skipping {var_type} variables as no interest variables are present\")\n",
    "                    continue # don't bother processing variables we are not interested in\n",
    "                if var_type == \"atmospheric_ds\" and (interest_levels is None or len(interest_levels)==0):\n",
    "                    verbose_print(verbose,f\" * Skipping atmospheric variables as no interest levels have been requested\")\n",
    "                    continue # we do not care about atmospheric variables if no levels are of interest\n",
    "                \n",
    "                # select interest variables and levels\n",
    "                vars_interest_variables = [var for var in vars_ds.data_vars if var in interest_variables]\n",
    "                if var_type == \"atmospheric_ds\":\n",
    "                    vars_ds = vars_ds[vars_interest_variables].sel(level=interest_levels)\n",
    "                else:\n",
    "                    vars_ds = vars_ds[vars_interest_variables]\n",
    "                    \n",
    "                # override time coordinates using the era5_base_frequency\n",
    "                vars_ds = vars_ds.assign_coords(\n",
    "                    {\"time\": pd.date_range(init_time+warmup_steps*pd.Timedelta(era5_base_frequency), \n",
    "                                           periods=vars_ds.sizes[\"time\"], \n",
    "                                           freq=era5_base_frequency)})\n",
    "                \n",
    "                # aggregate at eval_agg frequency\n",
    "                # use pd.Timedelta to avoid xarray automatically starting the resampling \n",
    "                # on Mondays for weekly etc.\n",
    "                # Note that resulting'time' will be the first timestamp in the aggregated period\n",
    "                vars_ds = vars_ds.resample(time=pd.Timedelta(eval_aggregation), origin=init_time).mean()\n",
    "                vars_ds = vars_ds.rename({\"time\": \"lead_time\"})\n",
    "                vars_ds[\"lead_time\"] = vars_ds[\"lead_time\"] - np.datetime64(init_time)\n",
    "                \n",
    "                # per-variable processing\n",
    "                for var in vars_ds.data_vars:\n",
    "                    # add lead time\n",
    "                    var_ds = vars_ds[var]\n",
    "                    \n",
    "                    # save\n",
    "                    path = f\"forecast_{var}_\" + \"-\".join([\n",
    "                        init_time.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                        str(era5_base_frequency),\n",
    "                        str(eval_aggregation),\n",
    "                        str(eval_start),\n",
    "                        str(forecast_horizon),\n",
    "                        str(var_ds.sizes[\"longitude\"])+ \"x\" +str(var_ds.sizes[\"latitude\"]),\n",
    "                    ]) + \".nc\"\n",
    "                    path = os.path.join(output_dir, path)\n",
    "                    verbose_print(verbose, f\"   * Saving new {var_type} forecast: {path}\")\n",
    "                    xr_to_netcdf(\n",
    "                        var_ds, path, \n",
    "                        precision=\"float32\", \n",
    "                        compression_level=1, \n",
    "                        sort_time=False, \n",
    "                        exist_ok=True\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
