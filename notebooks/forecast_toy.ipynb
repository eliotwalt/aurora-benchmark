{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewalt/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil\n",
    "import xarray as xr\n",
    "import torch\n",
    "import dask\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from aurora import Aurora, AuroraSmall, rollout\n",
    "\n",
    "from aurora_benchmark.utils import verbose_print, xr_to_netcdf\n",
    "\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "from aurora_benchmark.data import (\n",
    "    XRAuroraDataset, \n",
    "    aurora_batch_collate_fn, \n",
    "    aurora_batch_to_xr, \n",
    "    unpack_aurora_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x150f3849d240>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.nc\"\n",
    "ph5 = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.h5\"\n",
    "pzarr = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.zarr\"\n",
    "if os.path.exists(pzarr):\n",
    "    shutil.rmtree(pzarr)\n",
    "d = xr.open_dataset(p, engine=\"netcdf4\").drop_vars(\"time_bnds\")\n",
    "d.to_zarr(pzarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n",
      "/home/ewalt/.local/lib/python3.11/site-packages/xarray/core/dataset.py:273: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(FrozenMappingWarningOnValuesAccess({'latitude': 180, 'longitude': 360, 'time': 365}),\n",
       " FrozenMappingWarningOnValuesAccess({'level': 3, 'latitude': 180, 'longitude': 360, 'time': 365}),\n",
       " FrozenMappingWarningOnValuesAccess({'latitude': 180, 'longitude': 360}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era5_surface_paths = [\n",
    "    pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_atmospheric_paths = [ # we will repeat on level dimension\n",
    "    pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_static_paths = [ # we will select the first time step\n",
    "    pzarr, pzarr, pzarr\n",
    "]\n",
    "\n",
    "surf_vars = [\"2t\", \"msl\", \"10u\"]\n",
    "atmospheric_vars = [\"t\", \"q\", \"u\"]\n",
    "static_vars = [\"z\", \"lsm\", \"slt\"]\n",
    "\n",
    "# Load the data into a single dataset with the same coords but multiple variables\n",
    "surface_dss = [\n",
    "    #xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_surface_paths, surf_vars)\n",
    "]\n",
    "surface_ds = xr.merge(surface_dss)\n",
    "atmospheric_dss = [\n",
    "    #xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360, \"level\": 1}).rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_atmospheric_paths, atmospheric_vars)\n",
    "]\n",
    "atmospheric_ds = xr.merge(atmospheric_dss)\n",
    "static_dss = [\n",
    "    #\n",
    "    # xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    xr.open_zarr(path, chunks={\"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar}).isel(time=0)#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_static_paths, static_vars)\n",
    "]\n",
    "static_ds = xr.merge(static_dss)\n",
    "\n",
    "# rename coord lat to latitude and lon to longitude\n",
    "surface_ds = surface_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "atmospheric_ds = atmospheric_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "static_ds = static_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "surface_ds.dims, atmospheric_ds.dims, static_ds.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 356\n",
      "Dataloader length: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home3/ewalt/aurora-benchmark/aurora_benchmark/data.py:331: UserWarning: The init_frequency is not 1 day. This has not been tested.\n",
      "  warnings.warn(\"The init_frequency is not 1 day. This has not been tested.\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_workers = 2\n",
    "eval_start =\"3d\"\n",
    "era5_base_frequency = \"1d\"\n",
    "forecast_horizon = \"1w\"\n",
    "\n",
    "warmup_steps = int(pd.Timedelta(eval_start) / pd.Timedelta(era5_base_frequency)) if eval_start is not None else 0\n",
    "forecast_steps = int(pd.Timedelta(forecast_horizon) / pd.Timedelta(era5_base_frequency))\n",
    "\n",
    "dataset = XRAuroraDataset(\n",
    "    surface_ds=surface_ds,\n",
    "    atmospheric_ds=atmospheric_ds,\n",
    "    static_ds=static_ds,\n",
    "    init_frequency=\"1d\",\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "    drop_timestamps=True,\n",
    "    persist=False,\n",
    "    rechunk=False,\n",
    "    atmospheric_variables=atmospheric_vars,\n",
    "    surface_variables=surf_vars,\n",
    "    static_variables=static_vars,\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=aurora_batch_collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "print(\"Dataloader length:\", len(eval_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model ...\n",
      "Evaluating on cuda\n",
      "Rollout prediction on batch 0 ...\n",
      " * Rollout step 1: skipping warmup period\n",
      " * Rollout step 2: skipping warmup period\n",
      " * Rollout step 3: skipping warmup period\n",
      " * Rollout step 4: unpacked 1 sub-batches\n",
      " * Rollout step 5: unpacked 1 sub-batches\n",
      " * Rollout step 6: unpacked 1 sub-batches\n",
      " * Rollout step 7: unpacked 1 sub-batches\n",
      "Processing trajectories ...\n",
      " * init_time=2021-01-02 09:00:00: combining 4 steps\n",
      " * Skipping static variables\n",
      "Rollout prediction on batch 1 ...\n",
      " * Rollout step 1: skipping warmup period\n",
      " * Rollout step 2: skipping warmup period\n",
      " * Rollout step 3: skipping warmup period\n",
      " * Rollout step 4: unpacked 1 sub-batches\n",
      " * Rollout step 5: unpacked 1 sub-batches\n",
      " * Rollout step 6: unpacked 1 sub-batches\n",
      " * Rollout step 7: unpacked 1 sub-batches\n",
      "Processing trajectories ...\n",
      " * init_time=2021-01-03 09:00:00: combining 4 steps\n",
      " * Skipping static variables\n",
      "Rollout prediction on batch 2 ...\n",
      " * Rollout step 1: skipping warmup period\n",
      " * Rollout step 2: skipping warmup period\n",
      " * Rollout step 3: skipping warmup period\n",
      " * Rollout step 4: unpacked 1 sub-batches\n",
      " * Rollout step 5: unpacked 1 sub-batches\n",
      " * Rollout step 6: unpacked 1 sub-batches\n",
      " * Rollout step 7: unpacked 1 sub-batches\n",
      "Processing trajectories ...\n",
      " * init_time=2021-01-04 09:00:00: combining 4 steps\n",
      " * Skipping static variables\n",
      "Rollout prediction on batch 3 ...\n",
      " * Rollout step 1: skipping warmup period\n",
      " * Rollout step 2: skipping warmup period\n",
      " * Rollout step 3: skipping warmup period\n",
      " * Rollout step 4: unpacked 1 sub-batches\n",
      " * Rollout step 5: unpacked 1 sub-batches\n"
     ]
    }
   ],
   "source": [
    "AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {\n",
    "        \"u10\": \"10u\",\n",
    "        \"v10\": \"10v\",\n",
    "        \"t2m\": \"2t\",\n",
    "    },\n",
    "    \"atmospheric\": {},\n",
    "    \"static\": {},\n",
    "}\n",
    "INVERTED_AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"surface\"].items()},\n",
    "    \"atmospheric\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"atmospheric\"].items()},\n",
    "    \"static\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"static\"].items()},\n",
    "}\n",
    "\n",
    "interest_variables = atmospheric_vars + surf_vars\n",
    "interest_levels = [1000, 700, 250]\n",
    "\n",
    "\n",
    "verbose = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"loading model ...\")\n",
    "# aurora_model = \"aurora-0.25-pretrained.ckpt\"\n",
    "# model = Aurora(use_lora=False)\n",
    "# model.load_checkpoint(\"microsoft/aurora\", aurora_model)\n",
    "# model = model.to(device)\n",
    "\n",
    "print(f\"Evaluating on {device}\")\n",
    "# evaluation loop\n",
    "xr_preds = {\"surface_ds\": [], \"atmospheric_ds\": []}\n",
    "with torch.inference_mode() and torch.no_grad():\n",
    "    \n",
    "    ## MANUAL DATALOADING\n",
    "    \n",
    "    # print(f\"DEBUGGING: Manual dataloading\")\n",
    "    # batch_size = 1\n",
    "    # for i, batch in enumerate(dataset):\n",
    "        \n",
    "    \n",
    "    for i, batch in enumerate(dataset):\n",
    "        batch = batch.to(device)\n",
    "        # rollout until for forecast_steps\n",
    "        print(f\"Rollout prediction on batch {i} ...\")\n",
    "        trajectories = [[] for _ in range(batch_size)]\n",
    "        for s, batch_pred in enumerate(rollout(model, batch, steps=forecast_steps)):\n",
    "            if s < warmup_steps:\n",
    "                print(f\" * Rollout step {s+1}: skipping warmup period\")\n",
    "                continue            \n",
    "            # separate batched batches\n",
    "            sub_batch_preds = unpack_aurora_batch(batch_pred.to(\"cpu\"))\n",
    "            print(f\" * Rollout step {s+1}: unpacked {len(sub_batch_preds)} sub-batches\")\n",
    "            assert len(sub_batch_preds) == batch_size\n",
    "            # accumulate\n",
    "            for b, sub_batch_pred in enumerate(sub_batch_preds):\n",
    "                trajectories[b].append(sub_batch_pred)\n",
    "        print(f\"Processing trajectories ...\")\n",
    "        # convert to xr \n",
    "        for init_time, trajectory in zip(batch.metadata.time, trajectories):\n",
    "            print(f\" * init_time={init_time}: combining {len(trajectory)} steps\")\n",
    "            assert len(trajectory) == forecast_steps-warmup_steps\n",
    "            # collate trajectory batches\n",
    "            trajectory = aurora_batch_collate_fn(trajectory)\n",
    "            # convert to xr.Dataset\n",
    "            trajectory = aurora_batch_to_xr(trajectory, frequency=era5_base_frequency)\n",
    "            # add lead time\n",
    "            for var_type, vars_ds in trajectory.items():\n",
    "                # ensure processing is necessary\n",
    "                if var_type == \"static_ds\":\n",
    "                    print(f\" * Skipping static variables\")\n",
    "                    continue # we do not care about static variables for the forecast\n",
    "                if not any([var in vars_ds.data_vars for var in interest_variables]):\n",
    "                    print(f\" * Skipping {var_type} variables as no interest variables are present\")\n",
    "                    continue # don't bother processing variables we are not interested in\n",
    "                if var_type == \"atmospheric_ds\" and (interest_levels is None or len(interest_levels)==0):\n",
    "                    print(f\" * Skipping atmospheric variables as no interest levels have been requested\")\n",
    "                    continue # we do not care about atmospheric variables if no levels are of interest\n",
    "                \n",
    "                # select interest variables and levels\n",
    "                vars_interest_variables = [var for var in vars_ds.data_vars if var in interest_variables]\n",
    "                if var_type == \"atmospheric_ds\":\n",
    "                    vars_ds = vars_ds[vars_interest_variables].sel(level=interest_levels)\n",
    "                else:\n",
    "                    vars_ds = vars_ds[vars_interest_variables]\n",
    "                \n",
    "                # add lead time\n",
    "                vars_ds = vars_ds.assign_coords({\"lead_time\": vars_ds.time.values - np.datetime64(init_time)})\n",
    "                vars_ds = vars_ds.set_index({\"lead_time\": \"lead_time\"})\n",
    "                \n",
    "                # TODO: aggregate desired timesteps to agg freq  \n",
    "                # vars_ds = vars_ds.resample(time=eval_aggregation).mean()  # not enough as it messes with \"lead time\"                    \n",
    "                \n",
    "                # append to predictions\n",
    "                xr_preds[var_type].append(vars_ds)\n",
    "        \n",
    "# merge predictions and save\n",
    "for var_type, var_ds_list in xr_preds.items():\n",
    "    ds = xr.concat(var_ds_list, dim=\"time\").rename(INVERTED_AURORA_VARIABLE_RENAMES[var_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 2, 9, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = [batch, batch]\n",
    "batches[0].metadata.time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 4, 3, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_batch_preds[0].metadata.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
