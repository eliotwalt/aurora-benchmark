{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewalt/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil\n",
    "import xarray as xr\n",
    "import torch\n",
    "import dask\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from aurora import Aurora, AuroraSmall, rollout\n",
    "\n",
    "from aurora_benchmark.utils import verbose_print, xr_to_netcdf\n",
    "\n",
    "from aurora_benchmark.data import (\n",
    "    XRAuroraDataset, \n",
    "    XRAuroraBatchedDataset,\n",
    "    aurora_batch_collate_fn, \n",
    "    aurora_batch_to_xr, \n",
    "    unpack_aurora_batch\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Suppress logs from Google libraries\n",
    "logging.getLogger('google').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.auth').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.cloud').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x146cbed30fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.nc\"\n",
    "ph5 = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.h5\"\n",
    "pzarr = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.zarr\"\n",
    "if os.path.exists(pzarr):\n",
    "    shutil.rmtree(pzarr)\n",
    "d = xr.open_dataset(p, engine=\"netcdf4\").drop_vars(\"time_bnds\")\n",
    "d.to_zarr(pzarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenMappingWarningOnValuesAccess({'time': 365, 'longitude': 360, 'latitude': 180}),\n",
       " FrozenMappingWarningOnValuesAccess({'level': 3, 'time': 365, 'longitude': 360, 'latitude': 180}),\n",
       " FrozenMappingWarningOnValuesAccess({'longitude': 360, 'latitude': 180}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era5_surface_paths = [\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_atmospheric_paths = [ # we will repeat on level dimension\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_static_paths = [ # we will select the first time step\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "\n",
    "surf_vars = [\"2t\", \"msl\", \"10u\"]\n",
    "atmospheric_vars = [\"t\", \"q\", \"u\"]\n",
    "static_vars = [\"z\", \"lsm\", \"slt\"]\n",
    "\n",
    "# Load the data into a single dataset with the same coords but multiple variables\n",
    "surface_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_surface_paths, surf_vars)\n",
    "]\n",
    "surface_ds = xr.merge(surface_dss)\n",
    "atmospheric_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360, \"level\": 1}).rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_atmospheric_paths, atmospheric_vars)\n",
    "]\n",
    "atmospheric_ds = xr.merge(atmospheric_dss)\n",
    "static_dss = [\n",
    "     xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    # xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar}).isel(time=0)#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_static_paths, static_vars)\n",
    "]\n",
    "static_ds = xr.merge(static_dss)\n",
    "\n",
    "# rename coord lat to latitude and lon to longitude\n",
    "surface_ds = surface_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "atmospheric_ds = atmospheric_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "static_ds = static_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "surface_ds.dims, atmospheric_ds.dims, static_ds.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 11:50:45,942 - aurora_benchmark.utils - INFO - Using dask scheduler: threads\n",
      "2024-10-15 11:50:45,943 - aurora_benchmark.utils - INFO - Creating XRAuroraBatchedDataset ...\n",
      "2024-10-15 11:50:45,945 - aurora_benchmark.utils - INFO - Dataset length: 356\n",
      "2024-10-15 11:50:45,945 - aurora_benchmark.utils - INFO - Dataloader length: 89 (type: <class 'aurora_benchmark.data.XRAuroraBatchedDataset'>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_workers = 2\n",
    "eval_start =\"3d\"\n",
    "era5_base_frequency = \"1d\"\n",
    "forecast_horizon = \"1w\"\n",
    "use_dataloader = False\n",
    "eval_aggregation = \"1w\"\n",
    "init_frequency = \"3.5d\"\n",
    "verbose = True\n",
    "drop_timestamps = False\n",
    "persist = False\n",
    "rechunk = False\n",
    "output_dir = \"data/era5_wb2_forecasts/2021-2022-6h-1d-6w-1440x721_original_variables/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "warmup_steps = int(pd.Timedelta(eval_start) / pd.Timedelta(era5_base_frequency)) if eval_start is not None else 0\n",
    "forecast_steps = int(pd.Timedelta(forecast_horizon) / pd.Timedelta(era5_base_frequency))\n",
    "\n",
    "if use_dataloader:\n",
    "    dask.config.set(scheduler='synchronous')\n",
    "else:\n",
    "    dask.config.set(scheduler='threads')\n",
    "verbose_print(verbose, f\"Using dask scheduler: {dask.config.get('scheduler')}\")\n",
    "\n",
    "if use_dataloader:\n",
    "    verbose_print(verbose, f\"Creating XRAuroraDataset and DataLoader...\")\n",
    "    dataset = XRAuroraDataset(\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    verbose_print(verbose, f\"Loaded dataset of length {len(dataset)} (drop_timestamps={drop_timestamps}, persist={persist}, rechunk={rechunk})\")\n",
    "    \n",
    "    num_workers = 2 #int(os.getenv('SLURM_CPUS_PER_TASK', 1))+2 if os.getenv('SLURM_CPUS_PER_TASK') is not None else os.cpu_count()+2\n",
    "    verbose_print(verbose, f\"Creating DataLoader with {num_workers} workers ...\")\n",
    "    eval_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=aurora_batch_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    batch_iterator = eval_loader\n",
    "else:\n",
    "    # This is done to avoid the issue with torch DataLoader and dask\n",
    "    # when using netcdf files (i.e. netcdf backend is not thread safe)\n",
    "    verbose_print(verbose, f\"Creating XRAuroraBatchedDataset ...\")\n",
    "    dataset = XRAuroraBatchedDataset(\n",
    "        batch_size=batch_size,\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    batch_iterator = dataset\n",
    "\n",
    "verbose_print(verbose, f\"Dataset length: {dataset.flat_length() if hasattr(dataset, 'flat_length') else len(dataset)}\")\n",
    "verbose_print(verbose, f\"Dataloader length: {len(batch_iterator)} (type: {type(batch_iterator)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 11:51:03,738 - aurora_benchmark.utils - INFO - Evaluating on cuda\n",
      "2024-10-15 11:51:03,802 - aurora_benchmark.utils - INFO - Rollout prediction on batch 0 ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m verbose_print(verbose,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRollout prediction on batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[0;32m---> 38\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforecast_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m * Rollout step \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: skipping warmup period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/rollout.py:33\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(model, batch, steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(p\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 33\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pred\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Add the appropriate history so the model can be run on the prediction.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/aurora.py:183\u001b[0m, in \u001b[0;36mAurora.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    177\u001b[0m B, T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39msurf_vars\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    178\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    179\u001b[0m     batch,\n\u001b[1;32m    180\u001b[0m     static_vars\u001b[38;5;241m=\u001b[39m{k: v[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(B, T, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatic_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(\n\u001b[1;32m    188\u001b[0m     x,\n\u001b[1;32m    189\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    190\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    191\u001b[0m     rollout_step\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mrollout_step,\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    194\u001b[0m     x,\n\u001b[1;32m    195\u001b[0m     batch,\n\u001b[1;32m    196\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    197\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/encoder.py:254\u001b[0m, in \u001b[0;36mPerceiver3DEncoder.forward\u001b[0;34m(self, batch, lead_time)\u001b[0m\n\u001b[1;32m    252\u001b[0m absolute_time_encode \u001b[38;5;241m=\u001b[39m absolute_time_expansion(absolute_times, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    253\u001b[0m absolute_time_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsolute_time_embed(absolute_time_encode\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[0;32m--> 254\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mabsolute_time_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, L, D) + (B, 1, D)\u001b[39;00m\n\u001b[1;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {\n",
    "        \"u10\": \"10u\",\n",
    "        \"v10\": \"10v\",\n",
    "        \"t2m\": \"2t\",\n",
    "    },\n",
    "    \"atmospheric\": {},\n",
    "    \"static\": {},\n",
    "}\n",
    "INVERTED_AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"surface\"].items()},\n",
    "    \"atmospheric\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"atmospheric\"].items()},\n",
    "    \"static\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"static\"].items()},\n",
    "}\n",
    "\n",
    "interest_variables = atmospheric_vars + surf_vars\n",
    "interest_levels = [1000, 700, 250]\n",
    "\n",
    "\n",
    "verbose = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"loading model ...\")\n",
    "aurora_model = \"aurora-0.25-pretrained.ckpt\"\n",
    "model = Aurora(use_lora=False)\n",
    "model.load_checkpoint(\"microsoft/aurora\", aurora_model)\n",
    "model = model.to(device)\n",
    "\n",
    "verbose_print(verbose, f\"Evaluating on {device}\")\n",
    "# evaluation loop\n",
    "xr_preds = {\"surface_ds\": [], \"atmospheric_ds\": []}\n",
    "with torch.inference_mode() and torch.no_grad():\n",
    "    for i, batch in enumerate(batch_iterator):\n",
    "        batch = batch.to(device)\n",
    "        # rollout until for forecast_steps\n",
    "        verbose_print(verbose,f\"Rollout prediction on batch {i} ...\")\n",
    "        trajectories = [[] for _ in range(batch_size)]\n",
    "        for s, batch_pred in enumerate(rollout(model, batch, steps=forecast_steps)):\n",
    "            if s < warmup_steps:\n",
    "                verbose_print(verbose,f\" * Rollout step {s+1}: skipping warmup period\")\n",
    "                continue            \n",
    "            # separate batched batches\n",
    "            sub_batch_preds = unpack_aurora_batch(batch_pred.to(\"cpu\"))\n",
    "            verbose_print(verbose,f\" * Rollout step {s+1}: unpacked {len(sub_batch_preds)} sub-batches\")\n",
    "            assert len(sub_batch_preds) == batch_size\n",
    "            # accumulate\n",
    "            for b, sub_batch_pred in enumerate(sub_batch_preds):\n",
    "                trajectories[b].append(sub_batch_pred)\n",
    "        \n",
    "        # convert to xr and process\n",
    "        verbose_print(verbose,f\"Processing trajectories ...\")\n",
    "        for init_time, trajectory in zip(batch.metadata.time, trajectories):\n",
    "            verbose_print(verbose,f\" * init_time={init_time}: combining {len(trajectory)} steps\")\n",
    "            assert len(trajectory) == forecast_steps-warmup_steps\n",
    "            # collate trajectory batches\n",
    "            trajectory = aurora_batch_collate_fn(trajectory)\n",
    "            # convert to xr.Dataset\n",
    "            trajectory = aurora_batch_to_xr(trajectory, frequency=era5_base_frequency)\n",
    "            \n",
    "            # process individual trajectory elements (i.e. variable types)\n",
    "            for var_type, vars_ds in trajectory.items():\n",
    "                # ensure processing is necessary\n",
    "                if var_type == \"static_ds\":\n",
    "                    verbose_print(verbose,f\" * Skipping static variables\")\n",
    "                    continue # we do not care about static variables for the forecast\n",
    "                if not any([var in vars_ds.data_vars for var in interest_variables]):\n",
    "                    verbose_print(verbose,f\" * Skipping {var_type} variables as no interest variables are present\")\n",
    "                    continue # don't bother processing variables we are not interested in\n",
    "                if var_type == \"atmospheric_ds\" and (interest_levels is None or len(interest_levels)==0):\n",
    "                    verbose_print(verbose,f\" * Skipping atmospheric variables as no interest levels have been requested\")\n",
    "                    continue # we do not care about atmospheric variables if no levels are of interest\n",
    "                \n",
    "                # select interest variables and levels\n",
    "                vars_interest_variables = [var for var in vars_ds.data_vars if var in interest_variables]\n",
    "                if var_type == \"atmospheric_ds\":\n",
    "                    vars_ds = vars_ds[vars_interest_variables].sel(level=interest_levels)\n",
    "                else:\n",
    "                    vars_ds = vars_ds[vars_interest_variables]\n",
    "                \n",
    "                # aggregate at eval_agg frequency\n",
    "                # use pd.Timedelta to avoid xarray automatically starting the resampling \n",
    "                # on Mondays for weekly etc.\n",
    "                # Note that resulting'time' will be the first timestamp in the aggregated period\n",
    "                vars_ds = vars_ds.resample(time=pd.Timedelta(eval_aggregation), origin=init_time).mean()\n",
    "                \n",
    "                # per-variable processing\n",
    "                for var in vars_ds.data_vars:\n",
    "                    # add lead time\n",
    "                    var_ds = vars_ds[var].to_dataset(name=var)\n",
    "                    var_ds = var_ds.assign_coords({\"lead_time\": var_ds.time.values - np.datetime64(init_time)})\n",
    "                    var_ds = var_ds.set_index({\"lead_time\": \"lead_time\"})\n",
    "                    \n",
    "                    # save\n",
    "                    path = f\"forecast_{var}_\" + \"-\".join([\n",
    "                        init_time.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                        str(era5_base_frequency),\n",
    "                        str(eval_aggregation),\n",
    "                        str(eval_start),\n",
    "                        str(forecast_horizon),\n",
    "                        str(var_ds.sizes[\"longitude\"])+ \"x\" +str(var_ds.sizes[\"latitude\"]),\n",
    "                    ]) + \".nc\"\n",
    "                    path = os.path.join(output_dir, path)\n",
    "                    verbose_print(verbose, f\"   * Saving new {var_type} forecast: {path}\")\n",
    "                    xr_to_netcdf(\n",
    "                        var_ds, path, \n",
    "                        precision=\"float32\", \n",
    "                        compression_level=1, \n",
    "                        sort_time=False, \n",
    "                        exist_ok=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'atmospheric_vars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [batch\u001b[38;5;241m.\u001b[39msurf_vars, \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matmospheric_vars\u001b[49m, batch\u001b[38;5;241m.\u001b[39mstatic_vars]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(var, \u001b[38;5;28mvars\u001b[39m[var]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'atmospheric_vars'"
     ]
    }
   ],
   "source": [
    "batch = dataset[0]\n",
    "for vars in [batch.surf_vars, batch.atmospheric_vars, batch.static_vars]:\n",
    "    for var in vars:\n",
    "        print(var, vars[var].shape)\n",
    "        print(vars[var])\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
