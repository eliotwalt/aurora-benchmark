{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewalt/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, shutil\n",
    "import xarray as xr\n",
    "import torch\n",
    "import dask\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from aurora import Aurora, AuroraSmall, rollout\n",
    "\n",
    "from aurora_benchmark.utils import verbose_print, xr_to_netcdf\n",
    "\n",
    "from aurora_benchmark.data import (\n",
    "    XRAuroraDataset, \n",
    "    XRAuroraBatchedDataset,\n",
    "    aurora_batch_collate_fn, \n",
    "    aurora_batch_to_xr, \n",
    "    unpack_aurora_batch\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Suppress logs from Google libraries\n",
    "logging.getLogger('google').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.auth').setLevel(logging.ERROR)\n",
    "logging.getLogger('google.cloud').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x148e10428fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.nc\"\n",
    "ph5 = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.h5\"\n",
    "pzarr = \"../toy_data/era5-1d-360x180/msl-2021-2022-1d-360x180.zarr\"\n",
    "if os.path.exists(pzarr):\n",
    "    shutil.rmtree(pzarr)\n",
    "d = xr.open_dataset(p, engine=\"netcdf4\").drop_vars(\"time_bnds\")\n",
    "d.to_zarr(pzarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenMappingWarningOnValuesAccess({'time': 365, 'longitude': 360, 'latitude': 180}),\n",
       " FrozenMappingWarningOnValuesAccess({'level': 3, 'time': 365, 'longitude': 360, 'latitude': 180}),\n",
       " FrozenMappingWarningOnValuesAccess({'longitude': 360, 'latitude': 180}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era5_surface_paths = [\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_atmospheric_paths = [ # we will repeat on level dimension\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "era5_static_paths = [ # we will select the first time step\n",
    "    p, p, p\n",
    "    #pzarr, pzarr, pzarr\n",
    "]\n",
    "\n",
    "surf_vars = [\"2t\", \"msl\", \"10u\"]\n",
    "atmospheric_vars = [\"t\", \"q\", \"u\"]\n",
    "static_vars = [\"z\", \"lsm\", \"slt\"]\n",
    "\n",
    "# Load the data into a single dataset with the same coords but multiple variables\n",
    "surface_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_surface_paths, surf_vars)\n",
    "]\n",
    "surface_ds = xr.merge(surface_dss)\n",
    "atmospheric_dss = [\n",
    "    xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]}).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"time\": 50, \"latitude\": 180, \"longitude\": 360, \"level\": 1}).rename({\"msl\": svar}).expand_dims({\"level\": [1000, 700, 250]})#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_atmospheric_paths, atmospheric_vars)\n",
    "]\n",
    "atmospheric_ds = xr.merge(atmospheric_dss)\n",
    "static_dss = [\n",
    "     xr.open_dataset(path, engine=\"netcdf4\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    # xr.open_dataset(path, engine=\"h5netcdf\").rename({\"msl\": svar}).isel(time=0).drop_vars(\"time_bnds\")\n",
    "    #xr.open_zarr(path, chunks={\"latitude\": 180, \"longitude\": 360}).rename({\"msl\": svar}).isel(time=0)#.drop_vars(\"time_bnds\")\n",
    "    for path, svar in zip(era5_static_paths, static_vars)\n",
    "]\n",
    "static_ds = xr.merge(static_dss)\n",
    "\n",
    "# rename coord lat to latitude and lon to longitude\n",
    "surface_ds = surface_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "atmospheric_ds = atmospheric_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "static_ds = static_ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "surface_ds.dims, atmospheric_ds.dims, static_ds.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:24:52,828 - aurora_benchmark.utils - INFO - Using dask scheduler: threads\n",
      "2024-10-16 09:24:52,828 - aurora_benchmark.utils - INFO - Creating XRAuroraBatchedDataset ...\n",
      "2024-10-16 09:24:52,831 - aurora_benchmark.utils - INFO - Dataset length: 42\n",
      "2024-10-16 09:24:52,832 - aurora_benchmark.utils - INFO - Dataloader length: 3 (type: <class 'aurora_benchmark.data.XRAuroraBatchedDataset'>, batch_size: 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2_forecasts/2021-2022-1d-1w-10w-360x180_original_variables/\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_workers = 2\n",
    "eval_start =\"1w\"\n",
    "era5_base_frequency = \"1d\"\n",
    "forecast_horizon = \"10w\"\n",
    "use_dataloader = False\n",
    "eval_aggregation = \"1w\"\n",
    "init_frequency = \"1w\"\n",
    "verbose = True\n",
    "drop_timestamps = False\n",
    "persist = False\n",
    "rechunk = False\n",
    "output_dir = \"/projects/prjs0981/ewalt/aurora_benchmark/data/era5_wb2_forecasts/2021-2022-1d-1w-10w-360x180_original_variables/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "warmup_steps = int(pd.Timedelta(eval_start) / pd.Timedelta(era5_base_frequency)) if eval_start is not None else 0\n",
    "forecast_steps = int(pd.Timedelta(forecast_horizon) / pd.Timedelta(era5_base_frequency))\n",
    "\n",
    "assert (forecast_steps-warmup_steps) * pd.Timedelta(era5_base_frequency) >= pd.Timedelta(eval_aggregation), \"Evaluation steps must be at least as long as eval_aggregation\" \n",
    "\n",
    "if use_dataloader:\n",
    "    dask.config.set(scheduler='synchronous')\n",
    "else:\n",
    "    dask.config.set(scheduler='threads')\n",
    "verbose_print(verbose, f\"Using dask scheduler: {dask.config.get('scheduler')}\")\n",
    "\n",
    "\n",
    "if use_dataloader:\n",
    "    verbose_print(verbose, f\"Creating XRAuroraDataset and DataLoader...\")\n",
    "    dataset = XRAuroraDataset(\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    verbose_print(verbose, f\"Loaded dataset of length {len(dataset)} (drop_timestamps={drop_timestamps}, persist={persist}, rechunk={rechunk})\")\n",
    "    \n",
    "    num_workers = 2 #int(os.getenv('SLURM_CPUS_PER_TASK', 1))+2 if os.getenv('SLURM_CPUS_PER_TASK') is not None else os.cpu_count()+2\n",
    "    verbose_print(verbose, f\"Creating DataLoader with {num_workers} workers ...\")\n",
    "    eval_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=aurora_batch_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    batch_iterator = eval_loader\n",
    "else:\n",
    "    # This is done to avoid the issue with torch DataLoader and dask\n",
    "    # when using netcdf files (i.e. netcdf backend is not thread safe)\n",
    "    verbose_print(verbose, f\"Creating XRAuroraBatchedDataset ...\")\n",
    "    dataset = XRAuroraBatchedDataset(\n",
    "        batch_size=batch_size,\n",
    "        surface_ds=surface_ds,\n",
    "        atmospheric_ds=atmospheric_ds,\n",
    "        static_ds=static_ds,\n",
    "        init_frequency=init_frequency,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        num_time_samples=2, # Aurora has fixed history length of 2...\n",
    "        drop_timestamps=drop_timestamps,\n",
    "        persist=persist,\n",
    "        rechunk=rechunk,\n",
    "        atmospheric_variables=atmospheric_vars,\n",
    "        surface_variables=surf_vars,\n",
    "        static_variables=static_vars,\n",
    "    )\n",
    "    batch_iterator = dataset\n",
    "\n",
    "verbose_print(verbose, f\"Dataset length: {dataset.flat_length() if hasattr(dataset, 'flat_length') else len(dataset)}\")\n",
    "verbose_print(verbose, f\"Dataloader length: {len(batch_iterator)} (type: {type(batch_iterator)}, batch_size: {batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:25:08,324 - aurora_benchmark.utils - INFO - Evaluating on cuda\n",
      "2024-10-16 09:25:08,506 - aurora_benchmark.utils - INFO - Rollout prediction on batch 0 ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m verbose_print(verbose,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRollout prediction on batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[0;32m---> 37\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforecast_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m * Rollout step \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: skipping warmup period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/rollout.py:33\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(model, batch, steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(p\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 33\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pred\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Add the appropriate history so the model can be run on the prediction.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/aurora.py:183\u001b[0m, in \u001b[0;36mAurora.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    177\u001b[0m B, T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39msurf_vars\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    178\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    179\u001b[0m     batch,\n\u001b[1;32m    180\u001b[0m     static_vars\u001b[38;5;241m=\u001b[39m{k: v[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(B, T, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatic_vars\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(\n\u001b[1;32m    188\u001b[0m     x,\n\u001b[1;32m    189\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    190\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    191\u001b[0m     rollout_step\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mrollout_step,\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    194\u001b[0m     x,\n\u001b[1;32m    195\u001b[0m     batch,\n\u001b[1;32m    196\u001b[0m     lead_time\u001b[38;5;241m=\u001b[39mtimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    197\u001b[0m     patch_res\u001b[38;5;241m=\u001b[39mpatch_res,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/encoder.py:220\u001b[0m, in \u001b[0;36mPerceiver3DEncoder.forward\u001b[0;34m(self, batch, lead_time)\u001b[0m\n\u001b[1;32m    217\u001b[0m x_atmos \u001b[38;5;241m=\u001b[39m x_atmos \u001b[38;5;241m+\u001b[39m atmos_levels_embed  \u001b[38;5;66;03m# (B, C_A, L, D)\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Aggregate over pressure levels.\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m x_atmos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_atmos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, C_A, L, D) to (B, C, L, D)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Concatenate the surface level with the amospheric levels.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_surf\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), x_atmos), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/encoder.py:155\u001b[0m, in \u001b[0;36mPerceiver3DEncoder.aggregate_levels\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbcld->blcd\u001b[39m\u001b[38;5;124m\"\u001b[39m, latents)\n\u001b[1;32m    153\u001b[0m latents \u001b[38;5;241m=\u001b[39m latents\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B * L, C_A, D)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevel_agg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B * L, C, D)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munflatten(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sizes\u001b[38;5;241m=\u001b[39m(B, L))  \u001b[38;5;66;03m# (B, L, C, D)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblcd->bcld\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)  \u001b[38;5;66;03m# (B, C, L, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/perceiver.py:207\u001b[0m, in \u001b[0;36mPerceiverResampler.forward\u001b[0;34m(self, latents, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the module.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Latent features of shape `(B, L1, D1)`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, ff, ln1, ln2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# We use post-res-norm like in Swin v2 and most Transformer architectures these days.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# This empirically works better than the pre-norm used in the original Perceiver.\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m ln1(\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# HuggingFace suggests using non-residual attention in Perceiver might work better when\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# the semantics of the query and the output are different:\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m#   https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/models/perceiver/modeling_perceiver.py#L398\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     latents \u001b[38;5;241m=\u001b[39m attn_out \u001b[38;5;241m+\u001b[39m latents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_latent \u001b[38;5;28;01melse\u001b[39;00m attn_out\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aurora/model/perceiver.py:136\u001b[0m, in \u001b[0;36mPerceiverAttention.forward\u001b[0;34m(self, latents, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_kv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, L2, D1) to twice (B, L2, D)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l (h d) -> b h l d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mh), (q, k, v))\n\u001b[0;32m--> 136\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB H L1 D -> B L1 (H D)\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# (B, L1, D)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {\n",
    "        \"u10\": \"10u\",\n",
    "        \"v10\": \"10v\",\n",
    "        \"t2m\": \"2t\",\n",
    "    },\n",
    "    \"atmospheric\": {},\n",
    "    \"static\": {},\n",
    "}\n",
    "INVERTED_AURORA_VARIABLE_RENAMES = {\n",
    "    \"surface\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"surface\"].items()},\n",
    "    \"atmospheric\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"atmospheric\"].items()},\n",
    "    \"static\": {v: k for k, v in AURORA_VARIABLE_RENAMES[\"static\"].items()},\n",
    "}\n",
    "\n",
    "interest_variables = atmospheric_vars + surf_vars\n",
    "interest_levels = [1000, 700, 250]\n",
    "\n",
    "\n",
    "verbose = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"loading model ...\")\n",
    "aurora_model = \"aurora-0.25-pretrained.ckpt\"\n",
    "model = Aurora(use_lora=False)\n",
    "model.load_checkpoint(\"microsoft/aurora\", aurora_model)\n",
    "model = model.to(device)\n",
    "\n",
    "verbose_print(verbose, f\"Evaluating on {device}\")\n",
    "# evaluation loop\n",
    "with torch.inference_mode() and torch.no_grad():\n",
    "    for i, batch in enumerate(batch_iterator):\n",
    "        batch = batch.to(device)\n",
    "        # rollout until for forecast_steps\n",
    "        verbose_print(verbose,f\"Rollout prediction on batch {i} ...\")\n",
    "        trajectories = [[] for _ in range(batch_size)]\n",
    "        for s, batch_pred in enumerate(rollout(model, batch, steps=forecast_steps)):\n",
    "            if s < warmup_steps:\n",
    "                verbose_print(verbose,f\" * Rollout step {s+1}: skipping warmup period\")\n",
    "                continue            \n",
    "            # separate batched batches\n",
    "            sub_batch_preds = unpack_aurora_batch(batch_pred.to(\"cpu\"))\n",
    "            verbose_print(verbose,f\" * Rollout step {s+1}: unpacked {len(sub_batch_preds)} sub-batches\")\n",
    "            if i != len(batch_iterator) - 1: # the last batch may not be full\n",
    "                assert len(sub_batch_preds) == batch_size\n",
    "            # accumulate\n",
    "            for b, sub_batch_pred in enumerate(sub_batch_preds):\n",
    "                trajectories[b].append(sub_batch_pred)\n",
    "        \n",
    "        # convert to xr and process\n",
    "        verbose_print(verbose,f\"Processing trajectories ...\")\n",
    "        for init_time, trajectory in zip(batch.metadata.time, trajectories):\n",
    "            verbose_print(verbose,f\" * init_time={init_time}: combining {len(trajectory)} steps\")\n",
    "            assert len(trajectory) == forecast_steps-warmup_steps\n",
    "            # collate trajectory batches\n",
    "            trajectory = aurora_batch_collate_fn(trajectory)\n",
    "            # convert to xr.Dataset\n",
    "            trajectory = aurora_batch_to_xr(trajectory, frequency=era5_base_frequency)\n",
    "            \n",
    "            # process individual trajectory elements (i.e. variable types)\n",
    "            for var_type, vars_ds in trajectory.items():\n",
    "                # ensure processing is necessary\n",
    "                if var_type == \"static_ds\":\n",
    "                    verbose_print(verbose,f\" * Skipping static variables\")\n",
    "                    continue # we do not care about static variables for the forecast\n",
    "                if not any([var in vars_ds.data_vars for var in interest_variables]):\n",
    "                    verbose_print(verbose,f\" * Skipping {var_type} variables as no interest variables are present\")\n",
    "                    continue # don't bother processing variables we are not interested in\n",
    "                if var_type == \"atmospheric_ds\" and (interest_levels is None or len(interest_levels)==0):\n",
    "                    verbose_print(verbose,f\" * Skipping atmospheric variables as no interest levels have been requested\")\n",
    "                    continue # we do not care about atmospheric variables if no levels are of interest\n",
    "                \n",
    "                # select interest variables and levels\n",
    "                vars_interest_variables = [var for var in vars_ds.data_vars if var in interest_variables]\n",
    "                if var_type == \"atmospheric_ds\":\n",
    "                    vars_ds = vars_ds[vars_interest_variables].sel(level=interest_levels)\n",
    "                else:\n",
    "                    vars_ds = vars_ds[vars_interest_variables]\n",
    "                    \n",
    "                # override time coordinates using the era5_base_frequency\n",
    "                vars_ds = vars_ds.assign_coords(\n",
    "                    {\"time\": pd.date_range(init_time+warmup_steps*pd.Timedelta(era5_base_frequency), \n",
    "                                           periods=vars_ds.sizes[\"time\"], \n",
    "                                           freq=era5_base_frequency)})\n",
    "                \n",
    "                # aggregate at eval_agg frequency\n",
    "                # use pd.Timedelta to avoid xarray automatically starting the resampling \n",
    "                # on Mondays for weekly etc.\n",
    "                # Note that resulting'time' will be the first timestamp in the aggregated period\n",
    "                vars_ds = vars_ds.resample(time=pd.Timedelta(eval_aggregation), origin=init_time).mean()\n",
    "                vars_ds = vars_ds.rename({\"time\": \"lead_time\"})\n",
    "                vars_ds[\"lead_time\"] = vars_ds[\"lead_time\"] - np.datetime64(init_time)\n",
    "                \n",
    "                # per-variable processing\n",
    "                for var in vars_ds.data_vars:\n",
    "                    # add lead time\n",
    "                    var_ds = vars_ds[var]\n",
    "                    \n",
    "                    # save\n",
    "                    path = f\"forecast_{var}_\" + \"-\".join([\n",
    "                        init_time.strftime(\"%Y%m%dT%H%M%S\"),\n",
    "                        str(era5_base_frequency),\n",
    "                        str(eval_aggregation),\n",
    "                        str(eval_start),\n",
    "                        str(forecast_horizon),\n",
    "                        str(var_ds.sizes[\"longitude\"])+ \"x\" +str(var_ds.sizes[\"latitude\"]),\n",
    "                    ]) + \".nc\"\n",
    "                    path = os.path.join(output_dir, path)\n",
    "                    verbose_print(verbose, f\"   * Saving new {var_type} forecast: {path}\")\n",
    "                    xr_to_netcdf(\n",
    "                        var_ds, path, \n",
    "                        precision=\"float32\", \n",
    "                        compression_level=1, \n",
    "                        sort_time=False, \n",
    "                        exist_ok=True\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
